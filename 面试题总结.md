# 常见面试题总结

作者：xfcherish
链接：https://www.nowcoder.com/discuss/65323
来源：牛客网

（1）代码题（leetcode类型），主要考察数据结构和基础算法，以及代码基本功
虽然这部分跟机器学习，深度学习关系不大，但也是面试的重中之重。基本每家公司的面试都问了大量的算法题和代码题，即使是商汤、face++这样的深度学习公司，考察这部分的时间也占到了我很多轮面试的60%甚至70%以上。我去face++面试的时候，面试官是residual net，shuffle net的作者；但他们的面试中，写代码题依旧是主要的部分。

大部分题目都不难，基本是leetcode medium的难度。但是要求在现场白板编程，思路要流畅，能做到一次性Bug-free. 并且，一般都是要给出时间复杂度和空间复杂度最优的做法。对于少数难度很大的题，也不要慌张。一般也不会一点思路也没有，尽力给面试官展现自己的思考过程。面试官也会引导你，给一点小提示，沿着提示把题目慢慢做出来也是可以通过面试的。

以下是我所遇到的一些需要当场写出完整代码的题目：

<1> 二分查找。分别实现C++中的lower_bound和upper_bound.

<2> 排序。 手写快速排序，归并排序，堆排序都被问到过。

<3> 给你一个数组，求这个数组的最大子段积

时间复杂度可以到O(n)

<4> 给你一个数组，在这个数组中找出不重合的两段，让这两段的字段和的差的绝对值最小。

时间复杂度可以到O(n)

<5> 给你一个数组，求一个k值，使得前k个数的方差 + 后面n-k个数的方差最小

时间复杂度可以到O(n)

<6> 给你一个只由0和1组成的字符串，找一个最长的子串，要求这个子串里面0和1的数目相等。

时间复杂度可以到O(n)

<7> 给你一个数组以及一个数K， 从这个数组里面选择三个数，使得三个数的和小于等于K， 问有多少种选择的方法？

时间复杂度可以到O(n^2)

<8> 给你一个只由0和1组成的矩阵，找出一个最大的子矩阵，要求这个子矩阵是方阵，并且这个子矩阵的所有元素为1

时间复杂度可以到O(n^2)

<9> 求一个字符串的最长回文子串

时间复杂度可以到O(n) (Manacher算法)

<10> 在一个数轴上移动，初始在0点，现在要到给定的某一个x点， 每一步有三种选择，坐标加1，坐标减1，坐标乘以2，请问最少需要多少步从0点到x点。

<11> 给你一个集合，输出这个集合的所有子集。

<12> 给你一个长度为n的数组，以及一个k值（k < n) 求出这个数组中每k个相邻元素里面的最大值。其实也就是一个一维的max pooling

时间复杂度可以到O(n)

<13> 写一个程序，在单位球面上随机取点，也就是说保证随机取到的点是均匀的。

<14> 给你一个长度为n的字符串s，以及m个短串（每个短串的长度小于10）， 每个字符串都是基因序列，也就是说只含有A,T,C,G这四个字母。在字符串中找出所有可以和任何一个短串模糊匹配的子串。模糊匹配的定义，两个字符串长度相等，并且至多有两个字符不一样，那么我们就可以说这两个字符串是模糊匹配的。

<15> 其它一些描述很复杂的题这里就不列了。


（2）数学题或者"智力"题。

不会涉及特别高深的数学知识，一般就是工科数学（微积分，概率论，线性代数）和一些组合数学的问题。

下面是我在面试中被问到过的问题：

<1> 如果一个女生说她集齐了十二个星座的前男友，她前男友数量的期望是多少？

ps：这道题在知乎上有广泛的讨论，作为知乎重度用户我也看到过。如果一个女生说，她集齐了十二个星座的前男友，我们应该如何估计她前男友的数量？

<2> 两个人玩游戏。有n堆石头，每堆分别有a1, a2, a3.... an个石头，每次一个游戏者可以从任意一堆石头里拿走至少一个石头，也可以整堆拿走，但不能从多堆石头里面拿。无法拿石头的游戏者输，请问这个游戏是否有先手必胜或者后手必胜的策略？ 如果有，请说出这个策略，并证明这个策略能保证必胜。

<3> 一个一维数轴，起始点在原点。每次向左或者向右走一步，概率都是0.5. 请问回到原点的步数期望是多少？

<4> 一条长度为1的线段，随机剪两刀，求有一根大于0.5的概率。

<5> 讲一下你理解的矩阵的秩。低秩矩阵有什么特点？ 在图像处理领域，这些特点有什么应用？

<6> 讲一下你理解的特征值和特征向量。

<7> 为什么负梯度方向是使函数值下降最快的方向？简单数学推导一下


（3）机器学习基础

这部分建议参考周志华老师的《机器学习》。

下面是我在面试中被问到过的问题：

<1> 逻辑回归和线性回归对比有什么优点？

<2> 逻辑回归可以处理非线性问题吗？

<3> 分类问题有哪些评价指标？每种的适用场景。

<4> 讲一下正则化，L1和L2正则化各自的特点和适用场景。

<5> 讲一下常用的损失函数以及各自的适用场景。

<6> 讲一下决策树和随机森林

<7> 讲一下GBDT的细节，写出GBDT的目标函数。 GBDT和Adaboost的区别与联系

<8> 手推softmax loss公式

<9> 讲一下SVM, SVM与LR有什么联系。

<10>讲一下PCA的步骤。PCA和SVD的区别和联系

<11> 讲一下ensemble

<12> 偏差和方差的区别。ensemble的方法中哪些是降低偏差，哪些是降低方差？

...... 这部分问得太琐碎了，我能记起来的问题就这么多了。我的感觉，这部分问题大多数不是问得很深，所以不至于被问得哑口无言，总有得扯；但是要想给出一个特别深刻的回答，还是需要对机器学习的基础算法了解比较透彻。


（4）深度学习基础

这部分的准备，我推荐花书（Bengio的Deep learning）和 @魏秀参 学长的《解析卷积神经网络-深度学习实践手册》

下面是我在面试中被问到过的问题：

<1> 手推BP

<2> 手推RNN和LSTM结构

<3> LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失

<4> 讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？

<5> 梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？

<6> CNN和RNN的梯度消失是一样的吗？

<6> 有哪些防止过拟合的方法？

<7> 讲一下激活函数sigmoid，tanh，relu. 各自的优点和适用场景？

<8> relu的负半轴导数都是0，这部分产生的梯度消失怎么办？

<9> batch size对收敛速度的影响。

<10> 讲一下batch normalization

<11> CNN做卷积运算的复杂度。如果一个CNN网络的输入channel数目和卷积核数目都减半，总的计算量变为原来的多少？

<12> 讲一下AlexNet的具体结构，每层的作用

<13> 讲一下你怎么理解dropout，分别从bagging和正则化的角度

<14> data augmentation有哪些技巧？

<15> 讲一下你了解的优化方法，sgd, momentum, rmsprop, adam的区别和联系

<16> 如果训练的神经网络不收敛，可能有哪些原因？

<17> 说一下你理解的卷积核， 1x1的卷积核有什么作用？

........

同上，这部分的很多问题也是每个人都或多或少能回答一点，但要答得很好还是需要功底的。


（5）科研上的开放性问题

这部分的问题没有固定答案，也没法很好地针对性准备。功在平时，多读paper多思考，注意培养自己的insight和intuition

下面是我在面试中被问到过的问题：

<1> 选一个计算机视觉、深度学习、机器学习的子领域，讲一下这个领域的发展脉络，重点讲出各种新方法提出时的motivation，以及谈谈这个领域以后会怎么发展。

<2> 讲一下你最近看的印象比较深的paper

<3> 讲一下经典的几种网络结构， AlexNet， VGG，GoogleNet， Residual Net等等，它们各自最重要的contribution

<4> 你看过最近很火的XXX paper吗? 你对这个有什么看法？

......

（6） 编程语言、操作系统等方面的一些问题。

C++， Python， 操作系统，Linux命令等等。这部分问得比较少，但还是有的，不具体列了

（7）针对简历里项目/论文 / 实习的一些问题。

这部分因人而异，我个人的对大家也没参考价值，也不列了。
